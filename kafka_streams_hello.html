<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
        <title>Kevin Boone: Getting started with Kafka
Streams</title>
        <link rel="shortcut icon" href="https://kevinboone.me/img/favicon.ico">
        <meta name="msvalidate.01" content="894212EEB3A89CC8B4E92780079B68E9"/>
        <meta name="google-site-verification" content="DXS4cMAJ8VKUgK84_-dl0J1hJK9HQdYU4HtimSr_zLE" />
        <meta name="description" content="%%DESC%%">
        <meta name="author" content="Kevin Boone">
        <meta name="viewport" content="width=device-width; initial-scale=1; maximum-scale=1">
        <link rel="stylesheet" href="css/main.css">
	<script>
          window.onclick = function(event)
            {
            var m = document.getElementById ("menu-button");
            if (m != event.target)
              {
              var c = document.getElementById ("menu-toggle");
              if (c.checked)
                m.style.backgroundImage = "url('img/close.png')";
              else
                m.style.backgroundImage = "url('img/hamburger.png')";
              }
            }
	</script>
</head>


<body>

<div id="myname">
Kevin Boone
</div>

<input id="menu-toggle" type="checkbox" />
<label class='menu-button-container' for="menu-toggle">
<div class='menu-button' id='menu-button'></div>
</label>
<ul id="menu">
 <li><a class="menu_entry" href="index.html">Home</a></li>
 <li><a class="menu_entry" href="contact.html">Contact</a></li>
 <li><a class="menu_entry" href="cv.html">CV</a></li>
 <li><a class="menu_entry" href="software.html">Software</a></li>
 <li><a class="menu_entry" href="articles.html">Articles</a></li>
 <li><span><form id="search_form" method="get" action="https://duckduckgo.com/" target="_blank"><input type="text" name="q" placeholder="Search" size="5" id="search_input" /><button type="submit" id="search_submit">&#128269;</button><input type="hidden" name="sites" value="kevinboone.me" /><input type="hidden" name="kn" value="1" /></form></span></li>
</ul>

<div id="content">



<p></p>
<h1 id="getting-started-with-kafka-streams">Getting started with Kafka
Streams</h1>
<p><img src="img/kafka_logo.png" class="article-top-image" /></p>
<p>Kafka Streams is a Java library and framework for creating
applications that consume, process, and return Apache Kafka messages.
It’s conceptually similar to Apache Camel, but tightly coupled to Kafka,
and optimized for the kinds of operations that Kakfa clients typically
do. Kafka Streams is therefore less flexible than Camel, but probably
makes it easier to do Kafka-related things. Camel has a Kafka client of
its own, so an alternative to Kafka Streams would be to write a Camel
application with Kafka producers and consumers.</p>
<blockquote>
<p><strong>Note</strong><br />
There’s no need to understand Camel to follow this article. However,
because Camel and Kafka Streams have broadly similar applications, I’ll
point out similarities and differences from time to time.</p>
</blockquote>
<p>In this article I describe step-by-step how to deploy and run a
trivial Kafka Streams application, from the ground up. I’ll use nothing
but command-line tools and a text editor so, I hope, it’s clear what’s
going on.</p>
<p>The article is a somewhat expanded version of the first step of the
official <a
href="https://kafka.apache.org/36/documentation/streams/tutorial"
target="_blank">Kafka Streams tutorial</a>, and begins with the same
<code>Pipe.java</code> applcation. It then expands this application to
do some simple data processing. All the Pipe application does is copy
from one Kafka topic to another. This is shown conceptually in the
diagram below.</p>
<figure>
<img src="img/kafka_streams_hello_1.png" class="regular-inline-image"
alt="A conceptual diagram of the ‘Hello World’ of Kafka Streams applications" />
<figcaption aria-hidden="true">A conceptual diagram of the ‘Hello World’
of Kafka Streams applications</figcaption>
</figure>
<p>I’ll explain what a ‘topology’ is shortly.</p>
<h2 id="anatomy-of-a-simple-kafka-streams-application">Anatomy of a
simple Kafka Streams application</h2>
<blockquote>
<p><strong>Note</strong><br />
With a few exceptions, which I hope are obvious, when I don’t give
package names for Java classes, they are in the package
<code>org.apache.kafka.streams</code>.</p>
</blockquote>
<p>Running a basic Kafka Streams applications amounts to instantiating
KafkaStreams object, and calling <code>start()</code> on it. The
<code>start</code> method will not exit until something in the
application causes it to stop.</p>
<p>The sequence of processing steps that the application performs is
called, in Kafka Streams jargon, a ‘topology’. A Streams topology is
broadly similar to a Camel ‘context’ with a single ‘route’.</p>
<p>So if we have defined a toplogy, coding a basic Streams application
looks like this:</p>
<pre class="codeblock"><code>Topology topology = ...
Properties props = new Properties();
props.put(StreamsConfig.APPLICATION_ID_CONFIG, &quot;streams-pipe&quot;);
props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;localhost:9092&quot;);
// Set other properties...

KafkaStreams streams = new KafkaStreams (topology, props);
streams.start();</code></pre>
<p>There are many properties that might need to be set, but in almost
all cases you’ll need to set the application ID and the bootstrap server
– this, of course, identifies the Kafka broker(s) the application will
make first contact with.</p>
<p>The interesting part of programming Kafka Streams is defining the
toplogy. More on that later, once we’ve set everything up.</p>
<h2 id="prerequisites">Prerequisites</h2>
<p>In this article I assume that the reader has a working Kafka
installation. It doesn’t matter whether it’s a simple, local
installation with one broker, or a complex deployment with multiple
brokers in the cloud. However, I’m assuming that you have a way to run
the usual command-line utilities like
<code>kafka-console-producer.sh</code>. If you’re running a Linux
installation, you probably just need to</p>
<pre><code>cd /opt/kafka</code></pre>
<p>or wherever the installation is. In a Kubernetes environment you’ll
probably have to log into a running Kafka broker pod to get to the
necessary binaries, or install Kafka locally as well.</p>
<p>In any case, in the commands in this article, I assume that Kafka is
running on my development workstation and, where I need a Kafka
bootstrap address, it will always be <em>localhost:9092</em>.</p>
<p>To follow this example (and any of the official Kafka tutorials)
you’ll need Maven, or an IDE tool that can understand Maven
<code>pom.xml</code> files. Of course you’ll need a Java compiler –
anything from Java 1.8 onwards should work fine.</p>
<h2 id="installing-the-sample-application">Installing the sample
application</h2>
<p>Kafka provides a Maven archetype for constructing a trivial Kafka
Streams application. Use the archetype like this:</p>
<pre class="codeblock"><code>mvn archetype:generate \
    -DarchetypeGroupId=org.apache.kafka \
    -DarchetypeArtifactId=streams-quickstart-java \
    -DarchetypeVersion=3.6.1 \
    -DgroupId=me.kevinboone.apacheintegration \
    -DartifactId=kstreams_test \
    -Dversion=0.1 \
    -Dpackage=me.kevinboone.apacheintegration.kstreams_test</code></pre>
<p>I’ve used my own names here; of course you can use any names you
like. In this example, the application will be installed into the
directory <code>kstreams_test</code>.</p>
<p>The generated code will include a Maven <code>pom.xml</code>, some
logging configuration, and a few Java sources. We will be working with
<code>Pipe.java</code> which, if you ran the Maven archetype as I did,
will be in</p>
<pre><code>src/main/java/me/kevinboone/apacheintegration/kstreams_test</code></pre>
<p>Other than <code>Pipe.java</code> and <code>pom.xml</code>, none of
the other files generated by the archetype are relevant in this
example.</p>
<p><em>Important:</em> If you aren’t using Eclipse, edit the generated
<code>pom.xml</code> and comment out the configuration for the Eclipse
JDT compiler:</p>
<pre><code>    &lt;!--compilerId&gt;jdt&lt;/compilerId--&gt;</code></pre>
<p>If you look at <code>pom.xml</code>, you’ll see the only dependency
needed for a basic Kafka Streams application:</p>
<pre class="codeblock"><code>&lt;dependency&gt;
  &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
  &lt;artifactId&gt;kafka-streams&lt;/artifactId&gt;
  &lt;version&gt;${kafka.version}&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>Of course, this package has many sub-dependencies, including the
ordinary Kafka Java client.</p>
<blockquote>
<p><strong>Note</strong><br />
The version of the Kafka Streams library need not be the same as that of
the Kafka installation itself. Of course, there are advantages to
assuming that you use the latest versions of both.</p>
</blockquote>
<h2 id="setting-up-kafka">Setting up Kafka</h2>
<p>The <a
href="https://kafka.apache.org/23/documentation/streams/developer-guide/manage-topics.html#streams-developer-guide-topics-user">documentation</a>
is clear that Kafka topics used by a streams application must be created
administratively. The explanation given is that topics might be shared
between multiple applications, which might make different assumptions
about their structure. This is true of non-Streams applications as well
but, whether the explanation is convincing, a Kafka Streams application
will not auto-create topics even if Kafka is set up to allow it.</p>
<p>So we’ll need to create the topics that the sample application
uses:</p>
<pre><code>bin/kafka-topics.sh --create \
    --bootstrap-server localhost:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic streams-plaintext-input

bin/kafka-topics.sh --create \
    --bootstrap-server localhost:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic streams-pipe-output </code></pre>
<h2 id="looking-at-the-topology">Looking at the topology</h2>
<p>The Pipe application defines its topology like this:</p>
<pre><code>StreamsBuilder builder = new StreamsBuilder();

builder.stream(&quot;streams-plaintext-input&quot;)
          .to(&quot;streams-pipe-output&quot;);

Topology topology = builder.build();</code></pre>
<p>This is an example of what has become known as the “fluent builder”
pattern – each method call on the ‘builder’ object returns another
builder object which can have other methods called on it. So the
specification of the topology amounts to a chain of method calls. This
will become clearer, if it is not already, with more complex
examples.</p>
<blockquote>
<p><strong>Note</strong> Apache Camel also provides a ‘fluent builder’
method for defining Camel routes. Unlike Camel, however, a Streams
topology can’t have multiple <code>to(...)</code> elements. Streams can
send messages to multiple targets, but not like that.</p>
</blockquote>
<h2 id="first-run">First run</h2>
<p>We’ll need a Kafka producer and consumer to see the application doing
anything. We’ll send messages to the topic
<code>streams-plaintext-input</code>, and consume them from
<code>streams-pipe-output</code>. The application should pipe messages
from the one topic to the other.</p>
<p>A simple way to send and receive messages is to run the simple Kafka
console tools in two different sessions:</p>
<pre><code>./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic streams-pipe-output</code></pre>
<pre><code>./bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic streams-plaintext-input</code></pre>
<p>I run the <code>Pipe</code> application like this:</p>
<pre><code>mvn compile exec:java -Dexec.mainClass=me.kevinboone.apacheintegration.kstreams_test.Pipe</code></pre>
<p>With the application running, anything typed into the message
producer utility should appear at the message consumer. If it does, well
done, the Streams application is working.</p>
<h2 id="adding-a-simple-transformation">Adding a simple
transformation</h2>
<p>Let’s extend the trivial Streams topology to performa a simple
transformation – converting textual input to lower case.</p>
<p>Change the line</p>
<pre class="codeblock"><code>builder.stream(&quot;streams-plaintext-input&quot;)...</code></pre>
<p>to:</p>
<pre class="codeblock"><code>builder.stream (&quot;streams-plaintext-input&quot;)
  .mapValues (value-&gt;((String)value).toLowerCase())
  .to (&quot;streams-pipe-output&quot;);</code></pre>
<p>We’ve inserted a call to <code>mapValues()</code> between the message
consumptin and production.</p>
<p><code>mapValues</code> is a method that is defined to take one
parameter, which implements the <code>ValueMapper</code> interface. This
interface has one method, that takes one argument and returns one return
value. The lamda expression</p>
<pre><code>value-&gt;((String)value).toLowerCase()</code></pre>
<p>satisfies this interface, because it implicitly defines a method with
one parameter and one return value.</p>
<blockquote>
<p><strong>Note</strong><br />
<code>mapValues()</code> is broadly similar to Camel’s
<code>process()</code>. However, Kafka Streams also has a
<code>process()</code> method with a slightly different purpose.</p>
</blockquote>
<p>In short, <code>mapValues()</code> provides a Kafka message body to
an expression, and sets the new message body to the result of the
expression.</p>
<blockquote>
<p><strong>Note</strong><br />
The Kafka Streams API relies heavily on generic types and lambda
expressions. If you’re not familiar with how these work in Java, it
might be worth brushing up before digging into Kafka Streams beyond the
‘Hello, World’ level.</p>
</blockquote>
<p>When you run this example, and messages sent to
<code>streams-plaintext-input</code> should appear on
<code>streams-pipe-output</code> in only lower case.</p>
<blockquote>
<p><strong>Note</strong><br />
Kafka messages form key-value pairs. This example does not transform, or
even see, message keys. There is a <code>map()</code> method that
operates on keys, as well as message bodies.</p>
</blockquote>
<h2 id="adding-some-simple-logging">Adding some simple logging</h2>
<p>Perhaps we want to trace the way that a message is transformed in the
execution of a topology. To do that, we’ll need to ‘see’ the message
body, but not change it. We can use the <code>peek()</code> method for
that.</p>
<pre class="codeblock"><code>builder.stream (&quot;streams-plaintext-input&quot;)
  .peek ((key,value)-&gt;System.out.println (&quot;before: &quot; + value))
  .mapValues (value-&gt;((String)value).toLowerCase())
  .peek ((key,value)-&gt;System.out.println (&quot;after: &quot; + value))
  .to (&quot;streams-pipe-output&quot;);</code></pre>
<p>The <code>peek()</code> method takes a paramter of type
<code>ForeachAction</code>, which is an interface that specifies a
single method that takes two values, and returns nothing
(<code>void</code>). To satisfy this interface, our lambda expression
must take two arguments:</p>
<pre><code>  .peek ((key,value)-&gt;...)</code></pre>
<p>and call a method which returns <code>void</code> (as
<code>println()</code> does).</p>
<p>In this case, I’m only displaying the message body, so I don’t need
the <code>key</code> argument; but it has to be present, even if only as
a placeholder, or the lambda expression won’t match the interface.</p>
<p>Because the interface method is defined to return <code>void</code>,
the right-hand side of the lambda’s <code>-&gt;</code> must be a method
call that is defined to return <code>void</code> as well. So the
<code>peek()</code> method does not allow an expression that modifies
the key or the value.</p>
<p>When you run this example, you should see some logging output from
the application, something like this:</p>
<pre><code>before: Hello, World!
after: hello, world!</code></pre>
<h2 id="duplicating-and-filtering-the-message-stream">Duplicating and
filtering the message stream</h2>
<p>In this section I’ll describe some simple message processing
operations that could be added to the basic Streams application.</p>
<blockquote>
<p><strong>Note</strong><br />
I’m not giving explicit instructions on running these examples: the
changes are in the same part of the code as all those above and, I hope,
the results will be reasonably clear.</p>
</blockquote>
<p>If we want to duplicate a message to multiple topics, we can’t do
this:</p>
<pre class="codeblock"><code>builder.stream (&quot;input&quot;)
  .to (&quot;output1&quot;)
  .to (&quot;output2&quot;);</code></pre>
<p>The <code>to()</code> method is terminal in the fluent builder – it
doesn’t return anything we can call further methods on. This is,
perhaps, a little disconcerting to Camel programmers, because the Camel
builder does allow it.</p>
<p>We can, however, do this, which has the same effect:</p>
<pre class="codeblock"><code>KStream ks = builder.stream(&quot;streams-plaintext-input&quot;);
ks.to(&quot;streams-pipe-output&quot;);
ks.to(&quot;streams-pipe-output-long&quot;);</code></pre>
<p>This topology defintion will take the input from
<code>streams-plaintext-input</code> and send it to both
<code>streams-pipe-output</code> and
<code>streams-pipe-output-long</code>. The reason for the
<code>-long</code> naming should become clear shortly.</p>
<blockquote>
<p><strong>Note</strong><br />
It won’t always matter, but it’s worth bearing in mind that this method
of splitting the stream duplicates the message keys as well as the
values.</p>
</blockquote>
<p>Rather than a straight duplication, we can filter the messages, so
that some go to one topic, and some to others. In the example below,
messages whose bodies are ten characters in length or shorter go to
<code>streams-pipe-output-long</code>, while others go to
<code>streams-pipe-output</code>.</p>
<pre class="codeblock"><code>KStream&lt;String,String&gt; ks = builder.stream(&quot;streams-plaintext-input&quot;);
ks.filter((key,value)-&gt;(value.length() &lt;= 10).to (&quot;streams-pipe-output&quot;);
ks.filter((key,value)-&gt;(value.length() &gt; 10).to (&quot;streams-pipe-output-long&quot;);</code></pre>
<p>The <code>filter()</code> method takes a lambda expression that
evaluates to a boolean value – <code>true</code> to forward the message,
<code>false</code> to drop it.</p>
<p>When it comes to dynamic routing, that is, routing messages to topics
whose names are determined at runtime, it’s easy to make the same
mistake that Camel developers often make.</p>
<p>Suppose I want to route messages to a topic whose name depends on
(say) the current date. I have a method <code>getDateString()</code>
that returns the date in a suitable format. It’s tempting to write
something like this:</p>
<pre class="codeblock"><code>KStream&lt;String,String&gt; ks = builder.stream(&quot;streams-plaintext-input&quot;);
  .to (&quot;streams-pipe-output_&quot; + getDateString());</code></pre>
<p>This fails, for reasons that are not obvious. The reason it fails is
that the fluent builder pattern creates a topology only once. It may
well executes for the life of the application, but all the data it needs
to execute has to be provided at start-up time. The
<code>getDateString()</code> method will be executed only once and,
though the date changes, the argument to the <code>to()</code> was
evaluated only once.</p>
<p>Kafka Streams has a way around this problem (as Camel does). Rather
than initializing the <code>to()</code> method with a string, we
initialize it with a <code>TopicExtractor</code>. In practice, it seems
to be common to use a lambda expression to do this, rather than writing
a class that implements the <code>TopicExtractor</code> interface
explicitly. So we could do this:</p>
<pre class="codeblock"><code>KStream&lt;String,String&gt; ks = builder.stream(&quot;streams-plaintext-input&quot;);
  .to ((key, value, context)-&gt;&quot;streams-pipe-output_&quot; + getDateString());</code></pre>
<p>The lamba expression (which results in a specific method call on a
<code>TopicExtractor</code>) is executed for each message, not just at
start-up.</p>
<pre class="codeblock"><code>KStream&lt;String,String&gt; ks = builder.stream(&quot;streams-plaintext-input&quot;);
ks.split()
  .branch ( (key,value)-&gt;(value.length() &gt; 10),
    Branched.withConsumer 
      (stream-&gt;stream.to (&quot;streams-pipe-output-long&quot;)) )
  .defaultBranch ( 
    Branched.withConsumer 
      (stream-&gt;stream.to (&quot;streams-pipe-output&quot;)) );</code></pre>
<h2 id="closing-remarks">Closing remarks</h2>
<p>It’s probably not a surprise that this article only scratches the
surface of the Kafka Streams API, which is extensive. on stateful
operations, stream merging, partition and key management, error
handling, etc., etc. place to start. In <a
href="kafka_streams_hello_2.html">Part 2</a> I’ll tackle the first of
these – the stateful operations of counting and aggregation.</p>

<p><span class="footer-clearance-para"/></p>
</div>

<div id="footer">
<a href="rss.html"><img src="img/rss.png" width="24px" height="24px"/></a>
Categories: <a href="software_development-groupindex.html">software development</a>, <a href="Java-groupindex.html">Java</a>

<span class="last-updated">Jan 19 2024
</span>
</div>

</body>
</html>


